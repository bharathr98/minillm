{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d7a2774",
   "metadata": {},
   "source": [
    "# Attention is all you need!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81b47c7",
   "metadata": {},
   "source": [
    "In this notebook, we set up and explain an autoencoder (encoder + decoder) that uses the transformer architecture from [Attention Is All You Need](https://arxiv.org/abs/1706.03762) for the tiny shakespeare dataset. We set a few rules\n",
    "\n",
    "1. We are only allowed to use the paper\n",
    "2. We are only allowed to use PyTorch documentation\n",
    "\n",
    "Personal note - when writing this notebook, I allowed myself the following leeway \n",
    "\n",
    "If we are not able to solve something after spending 30 minutes on it then\n",
    "\n",
    "3. We can look at the annotated version of Attention Is All You Need by Sasha Rush"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f207dda",
   "metadata": {},
   "source": [
    "### Global variables and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b795e404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bea4dc9",
   "metadata": {},
   "source": [
    "The model requires a lot of hyperparameters. Their purpose is not immediately straightforward at this point (we haven't even looked at the model yet!). We will constantly mark where each parameter appears for the first time in the notebook. For now the reader can just run the cell below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "dd3f5d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL HYPERS --------------------------\n",
    "vocab_size = 65 # For character level tokenisation. Actual vocab size used in the paper is 320000\n",
    "context_length = 8\n",
    "dmodel = 32\n",
    "n_embd = dmodel\n",
    "mlp_feature_dim = 32\n",
    "h = 2\n",
    "N = 2\n",
    "p = 0.5\n",
    "# ---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8017ef0c",
   "metadata": {},
   "source": [
    "## Implementing transformers from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263d58f5",
   "metadata": {},
   "source": [
    "For any model, it is important to first define what the input looks like. Tranformers use a sequence of token ids as the input. The length of the sequence defines the `context_length` and denotes the number of tokens (context) that the network looks at in a given instance. Since we are dealing with character level tokenisation, the vocab size is 65 (jump ahead to [Data](##-Data) to see why).\n",
    "\n",
    "We also need a target output. This, following the paper, will be the input with all tokens shifted left and a new token. So for example if the target text is abcd, the input is abc and the output is bcd (because given abc we want the network to predict d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a8221a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([23, 18,  2, 23, 27, 52, 24, 53]),\n",
       " tensor([18,  2, 23, 27, 52, 24, 53, 56]))"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Produce a random context_length sized list of token ids\n",
    "Xtr = torch.randint(0, vocab_size,(context_length,))\n",
    "Ytr = torch.cat((Xtr[1:], torch.randint(0,vocab_size,(1,))))\n",
    "Xtr, Ytr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "9bb28dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53953252",
   "metadata": {},
   "source": [
    "We now have our dataset (which is just a dummy variable that we randomly initialised). Let us now try to understand how to implement the transformer. We are not going to keep it modular in this section. We are going to do everything in a dumb procedural way. After we figure out everything from the paper in this section is when we set up the PyTorch-ified version of transformers in the next section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869549a8",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f0a440",
   "metadata": {},
   "source": [
    "The first step is to embed these characters using an embedding matrix. This introduces a new hyperparamater `n_embd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "77341d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((vocab_size, n_embd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "fbd98b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xtr]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8275cf87",
   "metadata": {},
   "source": [
    "This part was easy because we have already done this. Now the next complication is the positional encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5d8971",
   "metadata": {},
   "source": [
    "### Positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941b0d63",
   "metadata": {},
   "source": [
    "A word can often take different meanings on the basis of where it appears in a sentence. This further extends to position of a word in a paragraph if we consider multiple sentences. Thus it helps to add information about the position of a word inside the context window.\n",
    "\n",
    "The paper uses the following way of including information about the position.\n",
    "\n",
    "We have for each example a 16 by 32 matrix of embeddings. To these embeddings, we add a 16 by 32 positional encoding matrix. The values the matrix takes are as follows\n",
    "$$\n",
    "pos_{(n,2m)} = \\sin\\left(\\frac{n}{10000^{2m/32}}\\right) \\\\ \n",
    "pos_{(n,2m+1)} = \\cos\\left(\\frac{n}{10000^{2m/32}}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f0fe3c",
   "metadata": {},
   "source": [
    "This is a really weird way of adding positional encoding in my opinion but for now we just stay with this. I would personally train an entire matrix of positional encodings as a trainable parameter of the model. However, the paper mentions that they tried training a positional encoding and the performance was comparable to using the above encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "3d648bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = torch.zeros((context_length, n_embd))\n",
    "position = torch.arange(0,context_length)\n",
    "divisionFactor = 10000**(-torch.arange(0, n_embd, 2)/32)\n",
    "pos[:, ::2] = torch.sin(position.view(-1,1) * divisionFactor)\n",
    "pos[:,1::2] = torch.cos(position.view(-1,1) * divisionFactor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "81557f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(emb + pos).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c7d445",
   "metadata": {},
   "source": [
    "### Attention heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87091110",
   "metadata": {},
   "source": [
    "Now time for the main meat of the paper, which is the attention head. \n",
    "\n",
    "The gist of what the attention head does is that it treats the entire context as a phonebook, each token as an entity (a person or a company for example) and wants to give each token the power to choose which phone number it wants to focus on based on certain characteristics of the entity.\n",
    "\n",
    "Put in simple words, we want to give each token the power to understand what role it plays. So for example let's take the sentence \"The fox ate the sheep\" and assume for a moment that the tokenisation we use is word-level. We want to give the token `sheep` the ability to choose which tokens to focus on to figure out its role in the sentence. We want to let each token ask a question (or query). For simplicity we assume that the question lies in a $d_k$ dimensional space. Each token also emits an answer to each of those questions (the key). Obviously, these two need to live in the same number of dimensions (you cannot have questions with no answers, and you cannot have answers to non-existent questions). Now each token also emits a phone number (value) which we assume to lie in a $d_v$ dimensional space. Based on the questions it asks and the answers it gets, it chooses to either ignore the phone number or give it the most importance. In other words, we want a weighted sum of all the values emitted by all the tokens in the context.\n",
    "\n",
    "Summarising, for each character we have a $d_k$ dimensional query vector $q$, a $d_k$ dimensional key vector $k$, and  a $d_v$ dimensional value vector $v$. We package them together into the matrices $Q$ of dimension $\\text{context_length}\\times d_k$, $K$ of dimension $\\text{context_length}\\times d_k$ and $V$ of dimension $\\text{context_length}\\times d_v$. Then the attention is calculated as \n",
    "$$\n",
    "\\text{Attention} = \\text{softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "$$\n",
    "\n",
    "It is evident that the above is a sensible operation because $QK^T$ produces a $\\text{context_length}\\times\\text{context_length}$ matrix, which can be multiplied in a sensible way with $V$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f779c0e4",
   "metadata": {},
   "source": [
    "Each Q, K, V is learnt from the data itself. So then we need three linear layers that map from the data to Q, K, V and then calculate the attention. We follow the paper and take them all to be `n_embd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "f00c13ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = torch.randn((n_embd,n_embd))\n",
    "K = torch.randn((n_embd,n_embd))\n",
    "V = torch.randn((n_embd,n_embd))\n",
    "\n",
    "def oldattention(x): # This was redefined in later parts of the notebook to make MHA easier to write\n",
    "    query = x @ Q\n",
    "    key = x @ K\n",
    "    value = x @ V\n",
    "    score = torch.softmax(query @ key.transpose(-2,-1), -2) / math.sqrt(n_embd) # The -2 in softmax tells PyTorch to take a softmax in the second last dimension\n",
    "    attention = score @ value\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "bc37bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "attentionOutput = oldattention(emb + pos) + (emb + pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a58179",
   "metadata": {},
   "source": [
    "Now that we have the attention head, we need to pass it through a MLP. We work with the architecture of the original paper of having a single hidden layer, followed by a ReLU, and then a second layer that brings it back to n_embd. This introduces a new hyperparameter which controls the dimensions of the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "ed1ba32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn((n_embd, hidden_dim))\n",
    "b1 = torch.randn((hidden_dim,))\n",
    "W2 = torch.randn((hidden_dim, n_embd))\n",
    "b2 = torch.randn((n_embd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "b158221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = torch.nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "c4506e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "firstLayer = relu(attentionOutput @ W1 + b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "62062b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "secondLayer = firstLayer @ W2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "59f11898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secondLayer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dbe4a0",
   "metadata": {},
   "source": [
    "I believe we have completely implemented one transformer block. What we have not figured out is the following - \n",
    "\n",
    "1. How to implement multi-head attention layer\n",
    "2. What we have done so far is take a 16 context length by 32 embedding dimension and done a few things on it. Now how do we convert the output to something that we can evaluate the loss on?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8bdb9c",
   "metadata": {},
   "source": [
    "### Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4599e715",
   "metadata": {},
   "source": [
    "I am not sure I understand why multi-head attention makes calculation of attention more cost-effective. I need to take out a paper pad and do complexity calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6f93ce",
   "metadata": {},
   "source": [
    "First let's start by a simple calculation of the complexity of computing a product of two matrices. Let's say I have a matrix $A$ of size $n\\times k$ and a matrix $B$ of size $k \\times m$. We can take an example to make the calculation easier to understand.\n",
    "\n",
    "\n",
    "$$\n",
    "A = \n",
    "\\begin{pmatrix}\n",
    "8 & 6 \\\\\n",
    "5 & 2 \\\\ \n",
    "9 & 2 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "B = \n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 3 & 1 \\\\ \n",
    "4 & 5 & 6 & 3 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Now let's say that every access to an element in a matrix is $O(1)$ and every multiplication of two floats is $O(1)$. So how many accesses and how many multiplications am I doing in $AB$?\n",
    "\n",
    "If you work through it, it is $O(m\\times n \\times k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8802f7",
   "metadata": {},
   "source": [
    "Are there quicker multiplication algorithms?\n",
    "\n",
    "I just checked the answer to that and it depends. If you have $n\\times k$ and $k\\times m$ then the only thing you can do is the complexity I calculated, which is $O(m\\times n \\times k)$. If it is a square matrix then we can bring it down to roughly $O(n^{2.37})$ (check Coppersmith-Winograd algorithm).\n",
    "\n",
    "So now let us work to understand the complexity of the self-attention heads. \n",
    "\n",
    "What are the dimensions of $Q$ and $K$? It is $n \\times d_k$ where $n$ is the context length. And so to multiply $Q$ and $K^T$ would be $O(n^2 d_k)$. Softmax and division are $O(n^2)$ because there are $n^2$ elements in $QK^T$. And now multiplying this to $V$ is $O(n^2 d_v)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d284ba",
   "metadata": {},
   "source": [
    "I think the point of multi-head attention is the following. Taking $h$ heads of $d_k/h$ and $d_v/h$ does not improve time complexity because you have to do $h$ number of $O(n^2 d_k/h)$ matrix multiplications so the time complexity is the exact same. What makes it more efficient is that you can parallelise these $h$ matrix multiplications so thus in-effect the time complexity is $O(n^2 d_k/h)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fca781",
   "metadata": {},
   "source": [
    "Before we start writing the code for MultiHeadAttention we need to modify a few things in the definition for attention so that we can make it more modular. We are going to define it to take the query, key, and value, and return the attention directly. Which means, we are going to do `x @ Q` outside the attention function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e326044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value):\n",
    "    dk = key.shape[-1]\n",
    "    assert query.shape[-1] == key.shape[-1]\n",
    "    assert key.shape[-2] == value.shape[-2]\n",
    "    score = torch.softmax(query @ key.transpose(-2,-1), -2) / math.sqrt(dk)\n",
    "    attention = score @ value\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cfd62c",
   "metadata": {},
   "source": [
    "Now we are going to first implement an attention layer which takes in $d_k$ and $d_v$ and does vanilla attention without any multihead business. The only difference now is that we are going to initiate with `nn.Linear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "bbca304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, context_length, dmodel):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(dmodel, context_length, bias=False) for _ in range(3)])\n",
    "    def forward(self, x):\n",
    "        query, key, value = [lin(x) for lin in self.linears]\n",
    "        return attention(query, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "23b96c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0142, -0.0229, -0.0261,  0.0085,  0.0872, -0.0189,  0.0293,  0.0028],\n",
       "        [ 0.0562, -0.0150, -0.0367,  0.0365,  0.3662, -0.0319,  0.1328,  0.0268],\n",
       "        [ 0.0440, -0.0378, -0.0066,  0.0503,  0.4968, -0.0609,  0.1498,  0.0540],\n",
       "        [-0.0045, -0.0285, -0.0168,  0.0075,  0.0808, -0.0218,  0.0104, -0.0178],\n",
       "        [ 0.0756, -0.0193,  0.0133,  0.0619,  0.4654, -0.0774,  0.1321,  0.0845],\n",
       "        [-0.0539, -0.0739, -0.0428,  0.0136,  0.2315, -0.0687,  0.0344, -0.1187],\n",
       "        [ 0.1744, -0.0834, -0.1005,  0.0647,  0.4040, -0.0753,  0.1209,  0.1623],\n",
       "        [ 0.0743, -0.0170, -0.1167,  0.0211,  0.2525,  0.0036,  0.1400, -0.0052]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentionhead = AttentionHead(context_length, n_embd)\n",
    "x = emb+pos\n",
    "attentionhead(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286cc626",
   "metadata": {},
   "source": [
    "Now that we have got this down, we need to replicate this to make the multihead attention class. Let's implement for the case where the input and output sequence are of the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "ba727a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "f1144c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, context_length, dmodel, h):\n",
    "        super().__init__()\n",
    "        assert dmodel % h == 0\n",
    "        self.context_length = context_length\n",
    "        self.dmodel = dmodel\n",
    "        self.h = h\n",
    "        self.dk = self.dmodel // self.h\n",
    "        self.linears = nn.ModuleList([nn.Linear(dmodel, context_length) for _ in range(4)]) # One extra because of final linear in MHA\n",
    "    def forward(self, x):\n",
    "        query, key, value = [lin(x).view(-1, self.h, self.context_length, self.dk) for lin in self.linears[:-1]]\n",
    "        attentionScore = attention(query, key, value).view(-1, self.context_length, self.dmodel)\n",
    "        return self.linears[-1](attentionScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "b05ce5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiheadattention = MultiHeadAttention(context_length, dmodel, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "a660fb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiheadattention.linears[2](emb).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1f3402",
   "metadata": {},
   "source": [
    "We have now pretty much absorbed most of the intricacies of the model. Now it is time to implemet the encoder and the decoder blocks as subclasses of the module class and then put them all together and match the number of parameters with the number of paramters in the paper. If that matches then we are golden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "369137d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, context_length, dmodel, h):\n",
    "        super().__init__()\n",
    "        assert dmodel % h == 0\n",
    "        self.context_length = context_length\n",
    "        self.dmodel = dmodel\n",
    "        self.h = h\n",
    "        self.dk = self.dmodel // self.h\n",
    "        self.linears = nn.ModuleList([nn.Linear(dmodel, context_length) for _ in range(4)]) # One extra because of final linear in MHA\n",
    "    def attention(self, query, key, value):\n",
    "        dk = key.shape[-1]\n",
    "        assert query.shape[-1] == key.shape[-1]\n",
    "        assert key.shape[-2] == value.shape[-2]\n",
    "        score = torch.softmax(query @ key.transpose(-2,-1), -2) / math.sqrt(dk)\n",
    "        attention = score @ value\n",
    "        return attention\n",
    "    def forward(self, x):\n",
    "        query, key, value = [lin(x).view(-1, self.h, self.context_length, self.dk) for lin in self.linears[:-1]]\n",
    "        attentionScore = self.attention(query, key, value).view(-1, self.context_length, self.dmodel)\n",
    "        return self.linears[-1](attentionScore)\n",
    "\n",
    "# class EncodeBlock(nn.Module):# This is currently un-normalised\n",
    "#     def __init__(self, context_length, dmodel, h):\n",
    "#         self.dmodel = dmodel\n",
    "#         self.h = h\n",
    "#         self.layers = [\n",
    "#             MultiHeadAttention(context_length, dmodel, h),\n",
    "#             nn.Linear(dmodel, mlp_feature_dim, bias=True),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(mlp_feature_dim, dmodel, bias=True)\n",
    "#         ]\n",
    "#     def forward(self, x):\n",
    "#         for layer in self.layers:\n",
    "#             x = layer(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "511e8005",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [210]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mEncodeBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [140]\u001b[0m, in \u001b[0;36mEncodeBlock.__init__\u001b[0;34m(self, context_length, dmodel, h)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdmodel \u001b[38;5;241m=\u001b[39m dmodel\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m=\u001b[39m h\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[0;32m---> 20\u001b[0m     \u001b[43mSublayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMultiHeadAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     21\u001b[0m     Sublayer(MLP(dmodel, mlp_feature_dim))\n\u001b[1;32m     22\u001b[0m ])\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'p'"
     ]
    }
   ],
   "source": [
    "encoder = EncodeBlock(10, 512, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd258f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff943777",
   "metadata": {},
   "source": [
    "One key missing/pain point here is that I have not implemented a residual connection. This needs handling. One way of implementing it is to define a sublayer that is initialised using a module as an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5409ecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sublayer(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.norm = None # Needs changing \n",
    "    def forward(self, x):\n",
    "        return x + module(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87777cf",
   "metadata": {},
   "source": [
    "So now we change the encode block in the following way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2444d977",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dmodel, mlp_feature_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(dmodel, mlp_feature_dim, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_feature_dim, dmodel, bias=True)\n",
    "                      ])\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class EncodeBlock(nn.Module):# This is currently un-normalised\n",
    "    def __init__(self, context_length, dmodel, h):\n",
    "        super().__init__()\n",
    "        self.dmodel = dmodel\n",
    "        self.h = h\n",
    "        self.layers = nn.ModuleList([\n",
    "            Sublayer(MultiHeadAttention(context_length, dmodel, h)),\n",
    "            Sublayer(MLP(dmodel, mlp_feature_dim))\n",
    "        ])\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb062968",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncodeBlock(10, 512, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6fdc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668adc1d",
   "metadata": {},
   "source": [
    "Now this defines properly a full encoder block. We need to finish the file step of this, which is to write the full encoder stack and then verify everything is working as intended. For this we need to define now a function that clones the encode block N times for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930231c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797caf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, N, context_length, dmodel, h):\n",
    "        super().__init__()\n",
    "        self.layers = clones(EncodeBlock(context_length, dmodel, h), N)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffae096",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(2,3,32,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577849e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8456f7f1",
   "metadata": {},
   "source": [
    "This I believe completes the implementation of the full encoder block. Now we collect them all together as a recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23489195",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, context_length, dmodel, h):\n",
    "        super().__init__()\n",
    "        assert dmodel % h == 0\n",
    "        self.context_length = context_length\n",
    "        self.dmodel = dmodel\n",
    "        self.h = h\n",
    "        self.dk = self.dmodel // self.h\n",
    "        self.linears = nn.ModuleList([nn.Linear(dmodel, context_length) for _ in range(4)]) # One extra because of final linear in MHA\n",
    "    def attention(self, query, key, value):\n",
    "        dk = key.shape[-1]\n",
    "        assert query.shape[-1] == key.shape[-1]\n",
    "        assert key.shape[-2] == value.shape[-2]\n",
    "        score = torch.softmax(query @ key.transpose(-2,-1), -2) / math.sqrt(dk)\n",
    "        attention = score @ value\n",
    "        return attention\n",
    "    def forward(self, x):\n",
    "        query, key, value = [lin(x).view(-1, self.h, self.context_length, self.dk) for lin in self.linears[:-1]]\n",
    "        attentionScore = self.attention(query, key, value).view(-1, self.context_length, self.dmodel)\n",
    "        return self.linears[-1](attentionScore)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dmodel, mlp_feature_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(dmodel, mlp_feature_dim, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_feature_dim, dmodel, bias=True)\n",
    "                      ])\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class Sublayer(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.norm = None # Needs changing \n",
    "    def forward(self, x):\n",
    "        return x + self.module(x)\n",
    "    \n",
    "class EncodeBlock(nn.Module):# This is currently un-normalised\n",
    "    def __init__(self, context_length, dmodel, h):\n",
    "        super().__init__()\n",
    "        self.dmodel = dmodel\n",
    "        self.h = h\n",
    "        self.layers = nn.ModuleList([\n",
    "            Sublayer(MultiHeadAttention(context_length, dmodel, h)),\n",
    "            Sublayer(MLP(dmodel, mlp_feature_dim))\n",
    "        ])\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, N, context_length, dmodel, h):\n",
    "        super().__init__()\n",
    "        self.layers = clones(EncodeBlock(context_length, dmodel, h), N)\n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0582fba8",
   "metadata": {},
   "source": [
    "First we need to make sure that whatever we have done here makes total sense. We are going to start with a dummy set of data and parameters and make sure one by one that we get what we should be getting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568fd8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERS --------------------------\n",
    "context_length = 4\n",
    "n_embd = 4\n",
    "dmodel = 4\n",
    "mlp_feature_dim = 32\n",
    "h = 2\n",
    "N = 2\n",
    "# ---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c1ee5b",
   "metadata": {},
   "source": [
    "First we start with what the input is supposed to look like. We have a context length of `context_length` and a vocab size of `v_size`. The encoder block does not deal with this directly. Instead, it deals with the embedded version of it, and thus what we want as an input is a tensor of dim `(context_length, n_embd)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de35ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn([context_length, n_embd])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05277ae7",
   "metadata": {},
   "source": [
    "Now we need to decide on the attention architecture. We are going to pick the dummy `dmodel = 4`. So the dimension of our query and key space is `dmodel`. The value space dimension is also `dmodel`. Here one thing is to note. The authors are working in a situation where `dmodel = n_embd` and we maintain that status quo. In case one wanted to work in a different setting, the query and key will be `dmodel` while the value space has to be `n_embd` for it to make sense. Remember, the query and key are intertwined and are kind of a lookup in the yellow pages, while the value space is the actual phone number. This **has** to be `n_embd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82895d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(N, context_length, dmodel, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2efd42",
   "metadata": {},
   "source": [
    "For now we work under the assumption that the random x we generated has already been masked with the positional encoding. This is something that needs to be implemented in the architecture *outside* the encoder block, and thus for this testing we assume this has been achieved.\n",
    "\n",
    "Now onto calculating the result of `encoder` and making sure it all works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72ffacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoderoutput = encoder(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd4969d",
   "metadata": {},
   "source": [
    "The first immediate and obvious check is to make sure the dimension remains the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc41840",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoderoutput.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda9e8a1",
   "metadata": {},
   "source": [
    "The above output is expected since we specifically set `view(-1, self.context_length, self.dmodel)` in the multi-head attention block, which adds the batch dimension. This will be useful and important when we have a non-trivial batch dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5fde63",
   "metadata": {},
   "source": [
    "Now we want to verify that the module is doing what we want it to do. Let's look at x and the components again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63fd75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7998ad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58084f86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "firstAttentionModule = encoder.layers[0].layers[0].module\n",
    "encoder.layers[0].layers[0](x) == x + firstAttentionModule(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10b0241",
   "metadata": {},
   "source": [
    "This achieves the required x + attention(x). We are currently not applying a layer norm. We need to do that. Now onto the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8463055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "firstMLPModule = encoder.layers[0].layers[1].module\n",
    "attentionOutput = x + firstAttentionModule(x)\n",
    "encoder.layers[0].layers[1](attentionOutput) == attentionOutput + firstMLPModule(attentionOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409d3b83",
   "metadata": {},
   "source": [
    "This also makes sense. As a final check, let's make sure that the MLP calculates precisely what we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dfe8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(firstMLPModule.layers[0](x).shape)\n",
    "print(firstMLPModule.layers[1](firstMLPModule.layers[0](x)) >= 0)\n",
    "firstMLPModule.layers[2](firstMLPModule.layers[1](firstMLPModule.layers[0](x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3969ebb",
   "metadata": {},
   "source": [
    "So finally we need to make sure that the full block calculates what we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4d3ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.layers[0](x) == attentionOutput + firstMLPModule(attentionOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d30b14e",
   "metadata": {},
   "source": [
    "Now if this block works then we can be assured that the full encoder, which is just these encoder blocks copied `N` times, should work. Thus the final output of the un-normalised encoder without dropouts is just `encoderoutput`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748204ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoderoutput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f56e9a",
   "metadata": {},
   "source": [
    "Now onto implementing the layer norm and dropouts. Layernorm is just like batchnorm but you normalise over the layer instead of normalising over the batch. The layernorm module in PyTorch takes the output dimensions (called normalised_shape in the documentation). This is a little tricky to understand from the documentation so let's try to do it ourselves and see what it spits out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c30f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "layerNormTrial = nn.LayerNorm(dmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8cce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.sum(-1, keepdims=True)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fad6743",
   "metadata": {},
   "outputs": [],
   "source": [
    "layerNormTrial(x).sum(-1, keepdims=True)/4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bb30f1",
   "metadata": {},
   "source": [
    "So this is normalising in the last dimension, which is the right thing to do. However this does raise a documentation question. How do I normalise instead in the context instead of the embedding space?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a113c0",
   "metadata": {},
   "source": [
    "Anyway, we need to add this normalisation to each sublayer. That is easily achieved by only changing the sublayer code while keeping the other parts unchanged. We did already anticipate that if you notice we had a `self.norm = None` in the definition of `Sublayer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cd9ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sublayer(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.norm = nn.LayerNorm(dmodel) \n",
    "    def forward(self, x):\n",
    "        return self.norm(x + self.module(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e0c639",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(N, context_length, dmodel, h)\n",
    "encoder.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f11e4ee",
   "metadata": {},
   "source": [
    "There is also a dropout that is employed. Let's have a look at that. The idea there is that at the forward pass, some of the elements of the input tensor are randomly set to zero with a probability that you control. So, time for a new hyperparameter. The paper sets as p = 0.1. A 10% dropout rate seems a little high to me but we can test the performance when we actually do a training run later.\n",
    "\n",
    "The position of the dropout is slightly unclear to me. They say \"We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized.\" My interpretation is the following code. Notice I apply dropout to `self.module(x)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43a8343",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sublayer(nn.Module):\n",
    "    def __init__(self, module, p):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.norm = nn.LayerNorm(dmodel) \n",
    "    def forward(self, x):\n",
    "        return self.norm(x + self.dropout(self.module(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e259016f",
   "metadata": {},
   "source": [
    "The full code now needs to be modified to incorporate this addition `p`. This is the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56857681",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, context_length, dmodel, h):\n",
    "        super().__init__()\n",
    "        assert dmodel % h == 0\n",
    "        self.context_length = context_length\n",
    "        self.dmodel = dmodel\n",
    "        self.h = h\n",
    "        self.dk = self.dmodel // self.h\n",
    "        self.linears = nn.ModuleList([nn.Linear(dmodel, context_length) for _ in range(4)]) # One extra because of final linear in MHA\n",
    "    def attention(self, query, key, value):\n",
    "        dk = key.shape[-1]\n",
    "        assert query.shape[-1] == key.shape[-1]\n",
    "        assert key.shape[-2] == value.shape[-2]\n",
    "        score = torch.softmax(query @ key.transpose(-2,-1), -2) / math.sqrt(dk)\n",
    "        attention = score @ value\n",
    "        return attention\n",
    "    def forward(self, x, mem = None):\n",
    "        query, key, value = [lin(x).view(-1, self.h, self.context_length, self.dk) for lin in self.linears[:-1]]\n",
    "        attentionScore = self.attention(query, key, value).view(-1, self.context_length, self.dmodel)\n",
    "        return self.linears[-1](attentionScore)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dmodel, mlp_feature_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(dmodel, mlp_feature_dim, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_feature_dim, dmodel, bias=True)\n",
    "                      ])\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class Sublayer(nn.Module):\n",
    "    def __init__(self, module, p):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.norm = nn.LayerNorm(dmodel) \n",
    "    def forward(self, x):\n",
    "        return self.norm(x + self.dropout(self.module(x)))\n",
    "    \n",
    "class EncodeBlock(nn.Module):# This is currently un-normalised\n",
    "    def __init__(self, context_length, dmodel, h, p):\n",
    "        super().__init__()\n",
    "        self.dmodel = dmodel\n",
    "        self.h = h\n",
    "        self.layers = nn.ModuleList([\n",
    "            Sublayer(MultiHeadAttention(context_length, dmodel, h), p),\n",
    "            Sublayer(MLP(dmodel, mlp_feature_dim), p)\n",
    "        ])\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, N, context_length, dmodel, h, p):\n",
    "        super().__init__()\n",
    "        self.layers = clones(EncodeBlock(context_length, dmodel, h, p), N)\n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27458fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERS --------------------------\n",
    "context_length = 4\n",
    "n_embd = 4\n",
    "dmodel = 4\n",
    "mlp_feature_dim = 32\n",
    "h = 2\n",
    "N = 2\n",
    "pdropout = 0.1\n",
    "# ---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43e48cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(N, context_length, dmodel, h, pdropout)\n",
    "encoder.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeec5b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43156c3a",
   "metadata": {},
   "source": [
    "Now it is time to write the decoder stack. The only change in this is the addition of one more sublayer where instead of having an attention block and a MLP, there are two attention blocks and an MLP. The first attention block does exactly what you expect. The second one is a little weird. It performs the attention on the output of the encoder. To be more precise, it takes computes the query and key from the output of the encoder and applies it to the values computed from the input to the decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dbd1e8",
   "metadata": {},
   "source": [
    "In order to achieve this, we introduce a new `mem` variable in forward. We will have to write the `DecodeBlock` in a slightly different way where we cannot use `x = lin(x) for lin in self.layers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e0a559",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, context_length, dmodel, h):\n",
    "        super().__init__()\n",
    "        assert dmodel % h == 0\n",
    "        self.context_length = context_length\n",
    "        self.dmodel = dmodel\n",
    "        self.h = h\n",
    "        self.dk = self.dmodel // self.h\n",
    "        self.linears = nn.ModuleList([nn.Linear(dmodel, context_length) for _ in range(4)]) # One extra because of final linear in MHA\n",
    "    def attention(self, query, key, value):\n",
    "        dk = key.shape[-1]\n",
    "        assert query.shape[-1] == key.shape[-1]\n",
    "        assert key.shape[-2] == value.shape[-2]\n",
    "        score = torch.softmax(query @ key.transpose(-2,-1), -2) / math.sqrt(dk)\n",
    "        attention = score @ value\n",
    "        return attention\n",
    "    def forward(self, x, mem = None):\n",
    "        if mem != None:\n",
    "            query, key = [lin(mem).view(-1, self.h, self.context_length, self.dk) for lin in self.linears[:2]]\n",
    "            value = self.linears[2](x).view(-1, self.h, self.context_length, self.dk)\n",
    "        else:\n",
    "            query, key, value = [lin(x).view(-1, self.h, self.context_length, self.dk) for lin in self.linears[:-1]]\n",
    "        attentionScore = self.attention(query, key, value).view(-1, self.context_length, self.dmodel)\n",
    "        return self.linears[-1](attentionScore)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dmodel, mlp_feature_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(dmodel, mlp_feature_dim, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_feature_dim, dmodel, bias=True)\n",
    "                      ])\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class Sublayer(nn.Module):\n",
    "    def __init__(self, module, p):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.norm = nn.LayerNorm(dmodel) \n",
    "    def forward(self, x, mem = None):\n",
    "        if mem != None:\n",
    "            self.norm(x + self.dropout(self.module(x, mem)))\n",
    "        return self.norm(x + self.dropout(self.module(x)))\n",
    "    \n",
    "class EncodeBlock(nn.Module):# This is currently un-normalised\n",
    "    def __init__(self, context_length, dmodel, h, p):\n",
    "        super().__init__()\n",
    "        self.dmodel = dmodel\n",
    "        self.h = h\n",
    "        self.layers = nn.ModuleList([\n",
    "            Sublayer(MultiHeadAttention(context_length, dmodel, h), p),\n",
    "            Sublayer(MLP(dmodel, mlp_feature_dim), p)\n",
    "        ])\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, N, context_length, dmodel, h, p):\n",
    "        super().__init__()\n",
    "        self.layers = clones(EncodeBlock(context_length, dmodel, h, p), N)\n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6851066",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecodeBlock(nn.Module):# This is currently un-normalised\n",
    "    def __init__(self, context_length, dmodel, h, p):\n",
    "        super().__init__()\n",
    "        self.dmodel = dmodel\n",
    "        self.h = h\n",
    "        self.layers = nn.ModuleList([\n",
    "            Sublayer(MultiHeadAttention(context_length, dmodel, h), p),\n",
    "            Sublayer(MultiHeadAttention(context_length, dmodel, h), p),\n",
    "            Sublayer(MLP(dmodel, mlp_feature_dim), p)\n",
    "        ])\n",
    "    def forward(self, x, mem):\n",
    "        x = self.layers[0](x)      # The attention layer with everything on decoder input\n",
    "        x = self.layers[1](x, mem) # The attention layer with Q and K on encoder output\n",
    "        x = self.layers[2](x)      # FFNet\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, N, context_length, dmodel, h, p):\n",
    "        super().__init__()\n",
    "        self.layers = clones(DecodeBlock(context_length, dmodel, h, p), N)\n",
    "    def forward(self, x, mem):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mem)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d180e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(N, context_length, dmodel, h, pdropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e027a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder(x, encoder(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d48732",
   "metadata": {},
   "source": [
    "We now have both, an encoder and a decoder. Time to put all of these together. We still have to implement as mask in the decoder. We can come back to it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc549c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec8b74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding:\n",
    "    def __init__(self, context_length, n_embd):\n",
    "        self.pos = torch.zeros((context_length, n_embd))\n",
    "        self.position = torch.arange(0,context_length)\n",
    "        self.divisionFactor = 10000**(-torch.arange(0, n_embd, 2)/32)\n",
    "        self.pos[:, ::2] = torch.sin(self.position.view(-1,1) * self.divisionFactor)\n",
    "        self.pos[:,1::2] = torch.cos(self.position.view(-1,1) * self.divisionFactor)\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, context_length, vocab_dim, n_embd, dmodel, mlp_feature_dim, h, N, pdropout):\n",
    "        super().__init__()\n",
    "        self.penc = PositionalEncoding(context_length, n_embd)\n",
    "        self.encoder = Encoder(N, context_length, dmodel, h, pdropout)\n",
    "        self.decoder = Decoder(N, context_length, dmodel, h, pdropout)\n",
    "        self.linear = nn.Linear(n_embd, vocab_dim)\n",
    "        self.softmax = nn.Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cbb88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dim = 128\n",
    "transformer = Transformer(context_length, vocab_dim, n_embd, dmodel, mlp_feature_dim, h, N, pdropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bde593",
   "metadata": {},
   "outputs": [],
   "source": [
    "tranformer.linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2316eec4",
   "metadata": {},
   "source": [
    "Now let us write down the actual hyperparameters from the paper and see if the parameters are of the same order of magnitude as their paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140cc531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERS --------------------------\n",
    "vocab_dim = 32000\n",
    "context_length = 1024\n",
    "n_embd = 512\n",
    "dmodel = 512\n",
    "mlp_feature_dim = 2048\n",
    "h = 8\n",
    "N = 6\n",
    "pdropout = 0.1\n",
    "# ---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ba61a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(context_length, vocab_dim, n_embd, dmodel, mlp_feature_dim, h, N, pdropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506e4af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in transformer.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eae6bf",
   "metadata": {},
   "source": [
    "This is significantly higher than the expected 65 million. What's going wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980040aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c723dbd",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bccdcbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
