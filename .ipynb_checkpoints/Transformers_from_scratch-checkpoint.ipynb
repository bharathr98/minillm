{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d7a2774",
   "metadata": {},
   "source": [
    "# Attention is all you need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b795e404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba6daab",
   "metadata": {},
   "source": [
    "We are going to try and implement Attention Is All You Need completely from scratch for tiny shakespeare. We will then compare it with Andrej's lectures and see where we went wrong and where we can improve\n",
    "\n",
    "The rules are - \n",
    "1. I am only allowed to use Attention Is All You Need\n",
    "2. I am only allowed to use PyTorch documentation\n",
    "\n",
    "If I am not able to solve something after spending 30 minutes on it then\n",
    "\n",
    "3. I can look at the annotated version of Attention Is All You Need by Sasha Rush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b77c7203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERS --------------------------\n",
    "context_length = 16\n",
    "n_embd = 32\n",
    "hidden_dim = 256\n",
    "mlp_feature_dim = 2048\n",
    "# ---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501a241a",
   "metadata": {},
   "source": [
    "## Dataset setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff913de0",
   "metadata": {},
   "source": [
    "One major deviation from the paper is that we are not training for translation tasks but instead we are going to train for next word prection. This is so that we follow Andrej's lecture later when we have finished the implementation on our own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa5bd2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-19 12:15:01--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1,1M) [text/plain]\n",
      "Saving to: ‘input.txt.1’\n",
      "\n",
      "input.txt.1         100%[===================>]   1,06M  --.-KB/s    in 0,09s   \n",
      "\n",
      "2024-07-19 12:15:01 (12,3 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee248f3",
   "metadata": {},
   "source": [
    "We are not at all going to use any tutorials for help, no matter how long this exercise takes\n",
    "\n",
    "First we want to clean the data and see what the vocabulary looks like. We are going to be doing character level generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8d15e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"input.txt\",'r').readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410de152",
   "metadata": {},
   "source": [
    "We are using ^ as the special character that denotes the beginning of the text. But this is a little confusing for me right now. I am going to work with sentences so that there is a clear beginning and a clear end. I think it might make sense to replace '\\n' with '^' but I am not completely sure. I am choosing to make that substitution at this point. Let's see how that goes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b720d0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleanedtext=[]\n",
    "for line in text:\n",
    "    cleanedtext.append(line[:-1] + '^')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6186273",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(list(set(list(''.join(cleanedtext)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffc22c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {}\n",
    "itos = {}\n",
    "for ix, c in enumerate(vocab):\n",
    "    stoi[c] = ix\n",
    "    itos[ix] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bed3fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(stoi)\n",
    "stoi['^']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c8aa6d",
   "metadata": {},
   "source": [
    "We need to now create our training splits. We imitate what we did before in makemore and define a function that takes a list and generates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c61b83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDataset(text):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for sentence in text:\n",
    "        chars = [39] * context_length\n",
    "        X.append(chars)\n",
    "        Y.append(stoi[sentence[0]])\n",
    "        for x,y in zip(sentence,sentence[1:]):\n",
    "            chars = chars[1:] + [stoi[x]]\n",
    "            X.append(chars)\n",
    "            Y.append(stoi[y])\n",
    "    return torch.tensor(X), torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7419441",
   "metadata": {},
   "source": [
    "For now we only work with the first three examples to be sure that we are doing the right thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b81ebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr, Ytr = buildDataset(cleanedtext[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "503980d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([62, 16]), torch.Size([62]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr.shape, Ytr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8017ef0c",
   "metadata": {},
   "source": [
    "## Implementing transformers from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53953252",
   "metadata": {},
   "source": [
    "We now have our dataset. Let us now try to understand how to implement the transformer. We are not going to keep it modular in this section. We are going to do everything in a dumb procedural way. After we figure out everything from the paper in this section is when we set up the PyTorch-ified version of transformers in the next section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869549a8",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f0a440",
   "metadata": {},
   "source": [
    "The first step is to embed these characters using an embedding matrix. This introduces a new hyperparamater. We will work with a vocab of 65 characters so let's work with an embedding of 32. Let's not do it as a class. We can build the class later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77341d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((vocab_size, n_embd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbd98b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([62, 16, 32])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xtr]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "771f9504",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = emb.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8275cf87",
   "metadata": {},
   "source": [
    "This part was easy because we have already done this. Now the next complication is the positional encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5d8971",
   "metadata": {},
   "source": [
    "### Positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941b0d63",
   "metadata": {},
   "source": [
    "It looks like this is what they are doing in the model. We have for each example a 16 by 32 matrix of embeddings. To these embeddings, we add a 16 by 32 positional encoding matrix. The values the matrix takes are as follows\n",
    "$$\n",
    "pos_{(n,2m)} = \\sin\\left(\\frac{n}{10000^{2m/32}}\\right) \\\\ \n",
    "pos_{(n,2m+1)} = \\cos\\left(\\frac{n}{10000^{2m/32}}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f0fe3c",
   "metadata": {},
   "source": [
    "This is a really weird way of adding positional encoding in my opinion but for now we just stay with this. I would personally train an entire matrix of positional encodings as a trainable parameter of the model. However, the paper mentions that they tried training a positional encoding and the performance was comparable to using the above encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d648bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = torch.zeros((context_length, n_embd))\n",
    "position = torch.arange(0,context_length)\n",
    "divisionFactor = 10000**(-torch.arange(0, n_embd, 2)/32)\n",
    "pos[:, ::2] = torch.sin(position.view(-1,1) * divisionFactor)\n",
    "pos[:,1::2] = torch.cos(position.view(-1,1) * divisionFactor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81557f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([62, 16, 32])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(emb + pos).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c7d445",
   "metadata": {},
   "source": [
    "### Attention heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87091110",
   "metadata": {},
   "source": [
    "Now time for the main meat of the paper, which is the attention head. \n",
    "\n",
    "What we want is that for each character I want a $d_k$ dimensional query vector $q$, a $d_k$ dimensional key vector $k$, and  a $d_v$ dimensional value vector $v$. We package them together into the matrices $Q$ of dimension $\\text{context_length}\\times d_k$, $K$ of dimension $\\text{context_length}\\times d_k$ and $V$ of dimension $\\text{context_length}\\times d_v$. Then the attention is calculated as \n",
    "$$\n",
    "\\text{Attention} = \\text{softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "$$\n",
    "\n",
    "It is evident that the above is a sensible operation because $QK^T$ produces a $\\text{context_length}\\times\\text{context_length}$ matrix, which can be multiplied in a sensible way with $V$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f779c0e4",
   "metadata": {},
   "source": [
    "I assume that each Q, K, V is calculated from the data itself. So then we need three linear layers that map from the data to Q, K, V and then calculate the attention. We follow the paper and take them all to be n_embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f00c13ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = torch.randn((n_embd,n_embd))\n",
    "K = torch.randn((n_embd,n_embd))\n",
    "V = torch.randn((n_embd,n_embd))\n",
    "\n",
    "def oldattention(x): # This was redefined in later parts of the notebook to make MHA easier to write\n",
    "    query = x @ Q\n",
    "    key = x @ K\n",
    "    value = x @ V\n",
    "    score = torch.softmax(query @ key.transpose(-2,-1), -2) / math.sqrt(n_embd) # The -2 in softmax tells PyTorch to take a softmax in the second last dimension\n",
    "    attention = score @ value\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc37bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "attentionOutput = oldattention(emb + pos) + (emb + pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a58179",
   "metadata": {},
   "source": [
    "Now that we have the attention head, we need to pass it through a MLP. We work with the architecture of the original paper of having a single hidden layer, followed by a ReLU, and then a second layer that brings it back to n_embd. This introduces a new hyperparameter which controls the dimensions of the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed1ba32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn((n_embd, hidden_dim))\n",
    "b1 = torch.randn((hidden_dim,))\n",
    "W2 = torch.randn((hidden_dim, n_embd))\n",
    "b2 = torch.randn((n_embd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b158221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = torch.nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4506e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "firstLayer = relu(attentionOutput @ W1 + b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62062b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "secondLayer = firstLayer @ W2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59f11898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([62, 16, 32])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secondLayer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dbe4a0",
   "metadata": {},
   "source": [
    "I believe we have completely implemented one transformer block. What we have not figured out is the following - \n",
    "\n",
    "1. How to implement multi-head attention layer\n",
    "2. What we have done so far is take a 16 context length by 32 embedding dimension and done a few things on it. Now how do we convert the output to something that we can evaluate the loss on?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8bdb9c",
   "metadata": {},
   "source": [
    "### Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4599e715",
   "metadata": {},
   "source": [
    "I am not sure I understand why multi-head attention makes calculation of attention more cost-effective. I need to take out a paper pad and do complexity calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6f93ce",
   "metadata": {},
   "source": [
    "First let's start by a simple calculation of the complexity of computing a product of two matrices. Let's say I have a matrix $A$ of size $n\\times k$ and a matrix $B$ of size $k \\times m$. We can take an example to make the calculation easier to understand.\n",
    "\n",
    "\n",
    "$$\n",
    "A = \n",
    "\\begin{pmatrix}\n",
    "8 & 6 \\\\\n",
    "5 & 2 \\\\ \n",
    "9 & 2 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "B = \n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 3 & 1 \\\\ \n",
    "4 & 5 & 6 & 3 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Now let's say that every access to an element in a matrix is $O(1)$ and every multiplication of two floats is $O(1)$. So how many accesses and how many multiplications am I doing in $AB$?\n",
    "\n",
    "If you work through it, it is $O(m\\times n \\times k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8802f7",
   "metadata": {},
   "source": [
    "Are there quicker multiplication algorithms?\n",
    "\n",
    "I just checked the answer to that and it depends. If you have $n\\times k$ and $k\\times m$ then the only thing you can do is the complexity I calculated, which is $O(m\\times n \\times k)$. If it is a square matrix then we can bring it down to roughly $O(n^{2.37})$ (check Coppersmith-Winograd algorithm).\n",
    "\n",
    "So now let us work to understand the complexity of the self-attention heads. \n",
    "\n",
    "What are the dimensions of $Q$ and $K$? It is $n \\times d_k$ where $n$ is the context length. And so to multiply $Q$ and $K^T$ would be $O(n^2 d_k)$. Softmax and division are $O(n^2)$ because there are $n^2$ elements in $QK^T$. And now multiplying this to $V$ is $O(n^2 d_v)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d284ba",
   "metadata": {},
   "source": [
    "I think the point of multi-head attention is the following. Taking $h$ heads of $d_k/h$ and $d_v/h$ does not improve time complexity because you have to do $h$ number of $O(n^2 d_k/h)$ matrix multiplications so the time complexity is the exact same. What makes it more efficient is that you can parallelise these $h$ matrix multiplications so thus in-effect the time complexity is $O(n^2 d_k/h)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fca781",
   "metadata": {},
   "source": [
    "Before we start writing the code for MultiHeadAttention we need to modify a few things in the definition for attention so that we can make it more modular. We are going to define it to take the query, key, and value, and return the attention directly. Which means, we are going to do `x @ Q` outside the attention function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e326044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value):\n",
    "    dk = key.shape[-1]\n",
    "    assert query.shape[-1] == key.shape[-1]\n",
    "    assert key.shape[-2] == value.shape[-2]\n",
    "    score = torch.softmax(query @ key.transpose(-2,-1), -2) / math.sqrt(dk)\n",
    "    attention = score @ value\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cfd62c",
   "metadata": {},
   "source": [
    "Now we are going to first implement an attention layer which takes in $d_k$ and $d_v$ and does vanilla attention without any multihead business. The only difference now is that we are going to initiate with `nn.Linear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bbca304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, context_length, dmodel):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(dmodel, context_length, bias=False) for _ in range(3)])\n",
    "    def forward(self, x):\n",
    "        query, key, value = [lin(x) for lin in self.linears]\n",
    "        return attention(query, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "23b96c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.9404e-01,  1.8992e-02, -1.9497e-01,  ...,  4.7052e-02,\n",
       "          -2.4679e-02,  1.3535e-01],\n",
       "         [ 2.9696e-01,  2.0132e-02, -1.9722e-01,  ...,  4.8251e-02,\n",
       "          -2.4215e-02,  1.3552e-01],\n",
       "         [ 2.7997e-01,  2.0348e-02, -1.8717e-01,  ...,  4.6161e-02,\n",
       "          -2.2416e-02,  1.2657e-01],\n",
       "         ...,\n",
       "         [ 3.6162e-01,  2.3561e-02, -2.3937e-01,  ...,  6.2885e-02,\n",
       "          -3.0029e-02,  1.5895e-01],\n",
       "         [ 3.3795e-01,  2.4011e-02, -2.2630e-01,  ...,  5.7633e-02,\n",
       "          -2.7245e-02,  1.4767e-01],\n",
       "         [ 2.8149e-01,  2.1858e-02, -1.9130e-01,  ...,  4.6985e-02,\n",
       "          -2.2302e-02,  1.2220e-01]],\n",
       "\n",
       "        [[ 2.4788e-01, -1.2092e-02, -2.0285e-01,  ...,  2.9129e-02,\n",
       "          -2.1644e-02,  1.5743e-01],\n",
       "         [ 2.5924e-01, -2.2086e-04, -1.9580e-01,  ...,  3.6876e-02,\n",
       "          -2.2070e-02,  1.4633e-01],\n",
       "         [ 2.5196e-01,  9.4358e-03, -1.7963e-01,  ...,  4.0344e-02,\n",
       "          -2.1049e-02,  1.2851e-01],\n",
       "         ...,\n",
       "         [ 3.0408e-01, -9.3139e-03, -2.3943e-01,  ...,  4.4865e-02,\n",
       "          -2.6846e-02,  1.7864e-01],\n",
       "         [ 2.9646e-01,  7.9289e-03, -2.1439e-01,  ...,  4.9641e-02,\n",
       "          -2.5454e-02,  1.5102e-01],\n",
       "         [ 3.6154e-01,  2.7308e-02, -2.3349e-01,  ...,  6.7996e-02,\n",
       "          -3.3867e-02,  1.7341e-01]],\n",
       "\n",
       "        [[ 1.5361e-01, -1.9430e-02, -1.7812e-01,  ..., -7.0710e-03,\n",
       "          -6.9858e-02,  1.9858e-01],\n",
       "         [ 1.7407e-01, -7.5999e-03, -1.6275e-01,  ...,  1.0291e-02,\n",
       "          -5.0866e-02,  1.6168e-01],\n",
       "         [ 1.7770e-01,  2.3984e-03, -1.4348e-01,  ...,  2.0789e-02,\n",
       "          -3.6572e-02,  1.2746e-01],\n",
       "         ...,\n",
       "         [ 2.1127e-01, -1.6525e-02, -1.8798e-01,  ...,  2.8033e-02,\n",
       "          -3.9033e-02,  1.6878e-01],\n",
       "         [ 3.3107e-01,  2.1841e-02, -2.0870e-01,  ...,  6.5594e-02,\n",
       "          -3.6361e-02,  1.6623e-01],\n",
       "         [ 1.1077e+00,  7.9124e-02, -7.4065e-01,  ...,  1.8715e-01,\n",
       "          -9.9056e-02,  5.1309e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.9840e-02,  1.3004e-02, -1.7021e-02,  ..., -1.8274e-02,\n",
       "           2.8639e-03,  1.5102e-02],\n",
       "         [ 2.5454e-02,  1.1199e-02, -4.4862e-02,  ..., -2.2904e-02,\n",
       "           5.3756e-02, -2.0787e-02],\n",
       "         [ 4.1456e-02,  9.3345e-03, -6.5696e-02,  ..., -2.2048e-02,\n",
       "           8.5355e-02, -3.7241e-02],\n",
       "         ...,\n",
       "         [ 7.8934e-02,  1.2910e-01, -2.2471e-02,  ..., -3.0598e-01,\n",
       "          -1.1723e-01,  3.0895e-01],\n",
       "         [-1.4872e-02,  5.2184e-02, -4.8931e-03,  ..., -1.0203e-03,\n",
       "          -2.7445e-02,  2.3849e-02],\n",
       "         [ 3.5184e-02,  9.5378e-03, -6.1292e-02,  ..., -2.8396e-02,\n",
       "           6.9673e-02, -3.5237e-02]],\n",
       "\n",
       "        [[ 1.0268e-02,  5.9149e-03, -3.9694e-02,  ..., -1.3463e-02,\n",
       "           3.0486e-02, -2.0096e-02],\n",
       "         [ 4.1993e-02,  1.2620e-02, -5.3210e-02,  ..., -7.5622e-03,\n",
       "           6.5559e-02, -2.4731e-02],\n",
       "         [ 1.9467e-01,  5.7777e-02, -4.0395e-01,  ..., -4.6755e-02,\n",
       "          -1.7408e-01,  1.2480e-01],\n",
       "         ...,\n",
       "         [-7.9952e-02,  5.6793e-02, -3.4286e-02,  ...,  2.7752e-02,\n",
       "          -9.4007e-02, -3.3243e-02],\n",
       "         [ 2.7389e-02,  9.7248e-03, -3.9916e-02,  ..., -2.2930e-02,\n",
       "           3.3314e-02, -1.1960e-02],\n",
       "         [ 2.2908e-02, -1.3385e-02, -1.5698e-01,  ..., -4.4155e-02,\n",
       "           1.5244e-01, -1.1329e-01]],\n",
       "\n",
       "        [[ 2.9404e-01,  1.8992e-02, -1.9497e-01,  ...,  4.7052e-02,\n",
       "          -2.4679e-02,  1.3535e-01],\n",
       "         [ 2.9696e-01,  2.0132e-02, -1.9722e-01,  ...,  4.8251e-02,\n",
       "          -2.4215e-02,  1.3552e-01],\n",
       "         [ 2.7997e-01,  2.0348e-02, -1.8717e-01,  ...,  4.6161e-02,\n",
       "          -2.2416e-02,  1.2657e-01],\n",
       "         ...,\n",
       "         [ 3.6162e-01,  2.3561e-02, -2.3937e-01,  ...,  6.2885e-02,\n",
       "          -3.0029e-02,  1.5895e-01],\n",
       "         [ 3.3795e-01,  2.4011e-02, -2.2630e-01,  ...,  5.7633e-02,\n",
       "          -2.7245e-02,  1.4767e-01],\n",
       "         [ 2.8149e-01,  2.1858e-02, -1.9130e-01,  ...,  4.6985e-02,\n",
       "          -2.2302e-02,  1.2220e-01]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentionhead = AttentionHead(context_length, n_embd)\n",
    "x = emb+pos\n",
    "attentionhead(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286cc626",
   "metadata": {},
   "source": [
    "Now that we have got this down, we need to replicate this to make the multihead attention class. Let's implement for the case where the input and output sequence are of the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f1144c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, context_length, dmodel, h):\n",
    "        super().__init__()\n",
    "        assert dmodel % h == 0\n",
    "        self.context_length = context_length\n",
    "        self.dmodel = dmodel\n",
    "        self.h = h\n",
    "        self.dk = self.dmodel // self.h\n",
    "        self.linears = nn.ModuleList([nn.Linear(dmodel, context_length) for _ in range(4)]) # One extra because of final linear in MHA\n",
    "    def forward(self, x):\n",
    "        query, key, value = [lin(x).view(-1, self.h, self.context_length, self.dk) for lin in self.linears[:-1]]\n",
    "        attentionScore = attention(query, key, value).view(-1, self.context_length, self.dmodel)\n",
    "        return self.linears[-1](attentionScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b05ce5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiheadattention = MultiHeadAttention(context_length, n_embd, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a660fb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0176,  0.1306,  0.1182,  ..., -0.1918, -0.3473, -0.0476],\n",
       "         [ 0.0176,  0.1306,  0.1182,  ..., -0.1918, -0.3473, -0.0476],\n",
       "         [ 0.0176,  0.1306,  0.1182,  ..., -0.1918, -0.3473, -0.0476],\n",
       "         ...,\n",
       "         [ 0.0176,  0.1306,  0.1182,  ..., -0.1918, -0.3473, -0.0476],\n",
       "         [ 0.0136,  0.1055,  0.1346,  ..., -0.1414, -0.3138, -0.0216],\n",
       "         [ 0.0065,  0.0591,  0.1507,  ..., -0.0978, -0.0883, -0.0401]],\n",
       "\n",
       "        [[ 0.0176,  0.1306,  0.1182,  ..., -0.1918, -0.3473, -0.0476],\n",
       "         [ 0.0176,  0.1306,  0.1182,  ..., -0.1918, -0.3473, -0.0476],\n",
       "         [ 0.0176,  0.1306,  0.1182,  ..., -0.1918, -0.3473, -0.0476],\n",
       "         ...,\n",
       "         [ 0.0176,  0.1306,  0.1182,  ..., -0.1918, -0.3473, -0.0476],\n",
       "         [ 0.1122,  0.0960,  0.1362,  ..., -0.0324, -0.1125, -0.0229],\n",
       "         [ 0.1358,  0.0532,  0.1155,  ...,  0.1099, -0.1108,  0.0348]],\n",
       "\n",
       "        [[ 0.0176,  0.1306,  0.1182,  ..., -0.1918, -0.3473, -0.0476],\n",
       "         [ 0.0176,  0.1306,  0.1182,  ..., -0.1918, -0.3473, -0.0476],\n",
       "         [ 0.0176,  0.1306,  0.1182,  ..., -0.1918, -0.3473, -0.0476],\n",
       "         ...,\n",
       "         [ 0.0065,  0.0591,  0.1507,  ..., -0.0978, -0.0883, -0.0401],\n",
       "         [ 0.1627,  0.1303,  0.1056,  ...,  0.0025, -0.2098,  0.0457],\n",
       "         [ 0.2223,  0.0682,  0.0703,  ..., -0.0120, -0.1798,  0.0652]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.1386,  0.1339,  0.0602,  ...,  0.0966, -0.2354,  0.0345],\n",
       "         [ 0.2260,  0.1084,  0.1080,  ...,  0.0252, -0.0534,  0.0767],\n",
       "         [ 0.1720,  0.0713,  0.0973,  ...,  0.0792, -0.1683,  0.0748],\n",
       "         ...,\n",
       "         [ 0.1685,  0.1313,  0.1094,  ...,  0.0723, -0.1671,  0.0608],\n",
       "         [ 0.1201,  0.1090,  0.1656,  ..., -0.0024, -0.1592, -0.0032],\n",
       "         [ 0.1435,  0.1131,  0.1243,  ...,  0.0038, -0.2294,  0.0124]],\n",
       "\n",
       "        [[ 0.1120,  0.0695,  0.0928,  ...,  0.0125, -0.1099,  0.0182],\n",
       "         [ 0.1165,  0.0686,  0.1228,  ...,  0.0466, -0.1730,  0.0624],\n",
       "         [ 0.0740, -0.0059,  0.1191,  ...,  0.0916, -0.2210,  0.0411],\n",
       "         ...,\n",
       "         [ 0.1073,  0.1378,  0.1635,  ...,  0.0357, -0.0527,  0.0275],\n",
       "         [ 0.1103,  0.0688,  0.1349,  ..., -0.0268, -0.2608, -0.0310],\n",
       "         [ 0.0259,  0.1239,  0.1136,  ..., -0.0940, -0.2264, -0.0481]],\n",
       "\n",
       "        [[ 0.1720,  0.0713,  0.0973,  ...,  0.0792, -0.1683,  0.0748],\n",
       "         [ 0.0908, -0.0482,  0.1160,  ...,  0.1855, -0.1748,  0.0793],\n",
       "         [ 0.1002,  0.1296,  0.1175,  ...,  0.0508, -0.1322,  0.0134],\n",
       "         ...,\n",
       "         [ 0.0176,  0.1306,  0.1182,  ..., -0.1918, -0.3473, -0.0476],\n",
       "         [ 0.0176,  0.1306,  0.1182,  ..., -0.1918, -0.3473, -0.0476],\n",
       "         [ 0.0176,  0.1306,  0.1182,  ..., -0.1918, -0.3473, -0.0476]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiheadattention(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5076c9",
   "metadata": {},
   "source": [
    "We have now pretty much absorbed most of the intricacies of the model. Now it is time to implemet the encoder and the decoder blocks as subclasses of the module class and then put them all together and match the number of parameters with the number of paramters in the paper. If that matches then we are golden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2f0c797",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, context_length, dmodel, h):\n",
    "        super().__init__()\n",
    "        assert dmodel % h == 0\n",
    "        self.context_length = context_length\n",
    "        self.dmodel = dmodel\n",
    "        self.h = h\n",
    "        self.dk = self.dmodel // self.h\n",
    "        self.linears = nn.ModuleList([nn.Linear(dmodel, context_length) for _ in range(4)]) # One extra because of final linear in MHA\n",
    "    def attention(self, query, key, value):\n",
    "        dk = key.shape[-1]\n",
    "        assert query.shape[-1] == key.shape[-1]\n",
    "        assert key.shape[-2] == value.shape[-2]\n",
    "        score = torch.softmax(query @ key.transpose(-2,-1), -2) / math.sqrt(dk)\n",
    "        attention = score @ value\n",
    "        return attention\n",
    "    def forward(self, x):\n",
    "        query, key, value = [lin(x).view(-1, self.h, self.context_length, self.dk) for lin in self.linears[:-1]]\n",
    "        attentionScore = self.attention(query, key, value).view(-1, self.context_length, self.dmodel)\n",
    "        return self.linears[-1](attentionScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1442584",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):# This is currently un-normalised\n",
    "    def __init__(self, context_length, dmodel, h):\n",
    "        self.dmodel = dmodel\n",
    "        self.h = h\n",
    "        self.layers = [\n",
    "            MultiHeadAttention(context_length, dmodel, h),\n",
    "            nn.Linear(dmodel, mlp_feature_dim, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_feature_dim, dmodel, bias=True)\n",
    "        ]\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf8384c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(10, 512, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8abf0c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MultiHeadAttention(\n",
       "   (linears): ModuleList(\n",
       "     (0-3): 4 x Linear(in_features=512, out_features=10, bias=True)\n",
       "   )\n",
       " ),\n",
       " Linear(in_features=512, out_features=2048, bias=True),\n",
       " ReLU(),\n",
       " Linear(in_features=2048, out_features=512, bias=True)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1fcb03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
